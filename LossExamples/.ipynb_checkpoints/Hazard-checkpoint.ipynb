{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import gluon, nd, autograd\n",
    "mx.random.seed(1)\n",
    "import pandas as pd\n",
    "\n",
    "from mxnet.gluon import nn\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon.loss import PoissonNLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../train.csv',skiprows=1,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_nonnumeric = df.select_dtypes(include=np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50999, 17), (50999,))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_y = df_wo_nonnumeric[1]\n",
    "np_train_y = df_train_y.values\n",
    "df_train_x = df_wo_nonnumeric.drop(labels=1,axis =1)\n",
    "np_train_x = df_train_x.values\n",
    "X = nd.array(np_train_x)\n",
    "Y  = nd.array(np_train_y)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = X.shape[0]\n",
    "num_features = X.shape[1]\n",
    "epochs = 10\n",
    "num_outputs = 1\n",
    "model_ctx = mx.cpu()\n",
    "data_ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples =  50999\n",
      "Number of input features =  17\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples = \", num_examples)\n",
    "print(\"Number of input features = \",num_features)\n",
    "X_mean = nd.mean(X, axis=0)\n",
    "X_SD = nd.array(np.std(X.asnumpy(),axis=0))\n",
    "X_nm =  (X - X_mean) / X_SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "# Genrate the training set usin ArrayDataSet\n",
    "training_set = gluon.data.DataLoader(gluon.data.ArrayDataset(X_nm,Y), shuffle=True, batch_size = batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dense(None -> 1, Activation(relu))\n",
       "  (1): Dense(None -> 1, linear)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "# Add a sequence of layers.\n",
    "net.add(\n",
    "    nn.Dense(1,activation='relu'),\n",
    "    nn.Dense(1)\n",
    ")\n",
    "    \n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_loss = PoissonNLLLoss(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating forward pass output of the model\n",
    "def poissonloss_hidden(X):\n",
    "    yhat1 = net(X);\n",
    "    yhat = nd.exp(yhat1)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Normal(sigma=1), ctx=mx.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.06})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 50\n",
    "loss_sequence = []\n",
    "loss_seq = []\n",
    "niter = 0\n",
    "epochslist = []\n",
    "num_batches = num_examples / batch_size\n",
    "smoothing_constant = 0.5\n",
    "moving_loss = 0\n",
    "\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    # inner loop\n",
    "    for i, (data, label) in enumerate(training_set):\n",
    "\n",
    "        with autograd.record():\n",
    "            output = poissonloss_hidden(data)\n",
    "            loss =  poisson_loss(None,output, label, from_logits=False, compute_full=False)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        niter +=1\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss\n",
    "\n",
    "        # correct the bias from the moving averages\n",
    "        est_loss = moving_loss/(1-(1-smoothing_constant)**niter)\n",
    "        loss_seq.append(est_loss)\n",
    "        cumulative_loss += nd.mean(loss).asscalar()\n",
    "    if e % 5 ==0:    \n",
    "        print(\"Epoch %s, loss: %s\" % (e, cumulative_loss / num_examples))\n",
    "    loss_sequence.append(cumulative_loss/num_examples)\n",
    "    epochslist.append(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
